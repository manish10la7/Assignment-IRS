{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# define ground truth and system output\n",
        "# actual top 10 ranks from the vsm (doc_id is the document id) [cite: 56]\n",
        "vsm_ranked_list_doc_ids = [1245, 1191, 1124, 1136, 571, 1190, 309, 341, 701, 1132]\n",
        "\n",
        "# assumed true relevance (ground truth for the query)\n",
        "# the key is the doc id\n",
        "relevance_map = {\n",
        "    1245: 3, # highly relevant\n",
        "    1191: 2, # relevant\n",
        "    1124: 1, # moderately relevant\n",
        "    1136: 0, 571: 0, 1190: 0, 309: 0, 341: 0, 701: 0, 1132: 0 # not relevant for this sample\n",
        "}\n",
        "binary_relevant_set = {doc_id for doc_id, score in relevance_map.items() if score > 0}\n",
        "total_relevant_docs = len(binary_relevant_set) # total relevant docs is 3"
      ],
      "metadata": {
        "id": "4MMxQjObyuIn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# precision and recall function\n",
        "def precision_recall_at_k(ranked_list, relevant_set, k):\n",
        "    \"\"\"calculates precision and recall at cut-off k.\"\"\"\n",
        "    # count relevant documents retrieved in top k\n",
        "    retrieved_at_k = ranked_list[:k]\n",
        "    relevant_retrieved = sum(1 for doc_id in retrieved_at_k if doc_id in relevant_set)\n",
        "\n",
        "    # precision@k: relevant retrieved / total retrieved\n",
        "    precision = relevant_retrieved / k\n",
        "\n",
        "    # recall@k: relevant retrieved / total relevant in corpus\n",
        "    recall = relevant_retrieved / total_relevant_docs if total_relevant_docs > 0 else 0\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "# mean average precision (map) function\n",
        "def average_precision(ranked_list, relevant_set):\n",
        "    \"\"\"calculates average precision for a single query.\"\"\"\n",
        "    sum_precisions = 0\n",
        "    relevant_count = 0\n",
        "    for k, doc_id in enumerate(ranked_list, 1):\n",
        "        if doc_id in relevant_set:\n",
        "            relevant_count += 1\n",
        "            # precision at the rank of the current relevant document\n",
        "            precision_at_k = relevant_count / k\n",
        "            sum_precisions += precision_at_k\n",
        "\n",
        "    # ap = sum of precision at relevant ranks / total relevant documents\n",
        "    return sum_precisions / total_relevant_docs if total_relevant_docs > 0 else 0\n",
        "\n",
        "# normalized discounted cumulative gain (ndcg) function\n",
        "def ndcg_at_k(ranked_list, relevance_map, k):\n",
        "    \"\"\"calculates ndcg at cut-off k using graded relevance.\"\"\"\n",
        "\n",
        "    # extract relevance scores for ranked list\n",
        "    rel_scores = np.array([relevance_map.get(doc_id, 0) for doc_id in ranked_list[:k]])\n",
        "\n",
        "    # dcg (discounted cumulative gain)\n",
        "    # dcg formula: $\\text{dcg} = \\sum_{i=1}^{k} \\frac{2^{\\text{rel}_i} - 1}{\\log_2(i+1)}$\n",
        "    gain = 2**rel_scores - 1\n",
        "    # add 2 because rank starts at 1, so log2(1+1) = log2(2) = 1\n",
        "    discounts = np.log2(np.arange(2, len(rel_scores) + 2))\n",
        "    dcg = np.sum(gain / discounts)\n",
        "\n",
        "    # idcg (ideal discounted cumulative gain)\n",
        "    # sort true relevance scores ideally\n",
        "    ideal_scores = np.array(sorted(relevance_map.values(), reverse=True))\n",
        "    ideal_scores_k = ideal_scores[:k]\n",
        "\n",
        "    ideal_gain = 2**ideal_scores_k - 1\n",
        "    idcg = np.sum(ideal_gain / discounts)\n",
        "\n",
        "    # ndcg = dcg / idcg\n",
        "    return dcg / idcg if idcg > 0 else 0"
      ],
      "metadata": {
        "id": "cUtp33L__mKc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation execution\n",
        "K_value = 5 # evaluate performance for top 5 results\n",
        "P_at_K, R_at_K = precision_recall_at_k(vsm_ranked_list_doc_ids, binary_relevant_set, K_value)\n",
        "AP_score = average_precision(vsm_ranked_list_doc_ids, binary_relevant_set)\n",
        "nDCG_at_K = ndcg_at_k(vsm_ranked_list_doc_ids, relevance_map, K_value)\n",
        "\n",
        "print(\"\\n--- VSM Evaluation Metrics Output (Sample) ---\")\n",
        "print(f\"Total Relevant Docs (in sample): {total_relevant_docs}\")\n",
        "print(f\"Precision@{K_value}: {P_at_K:.4f}\")\n",
        "print(f\"Recall@{K_value}: {R_at_K:.4f}\")\n",
        "print(f\"Average Precision (AP) for Query: {AP_score:.4f}\")\n",
        "print(f\"nDCG@{K_value}: {nDCG_at_K:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9O-4W-M_wQr",
        "outputId": "f7c709eb-ede8-4f3b-b5e0-e8b1a7735b19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- VSM Evaluation Metrics Output (Sample) ---\n",
            "Total Relevant Docs (in sample): 3\n",
            "Precision@5: 0.6000\n",
            "Recall@5: 1.0000\n",
            "Average Precision (AP) for Query: 1.0000\n",
            "nDCG@5: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuSuLL-X_x52"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}