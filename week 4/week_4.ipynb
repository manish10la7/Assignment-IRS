{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "42AuZMwGst_9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    return text.split()\n",
        "def load_documents():\n",
        "    files = [\n",
        "        \"bm25_vs_jm.txt\",\n",
        "        \"language_model_jm.txt\",\n",
        "        \"okapi_bm25.txt\"\n",
        "    ]\n",
        "\n",
        "    documents = {}\n",
        "    for f in files:\n",
        "        with open(f, \"r\", encoding=\"utf-8\") as file:\n",
        "            documents[f] = preprocess(file.read())\n",
        "    return documents"
      ],
      "metadata": {
        "id": "O3dPo-6Tszqq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_stats(documents):\n",
        "    tf = defaultdict(lambda: defaultdict(int))\n",
        "    df = defaultdict(int)\n",
        "    doc_len = {}\n",
        "\n",
        "    for doc, tokens in documents.items():\n",
        "        doc_len[doc] = len(tokens)\n",
        "        unique_terms = set(tokens)\n",
        "\n",
        "        for term in tokens:\n",
        "            tf[term][doc] += 1\n",
        "\n",
        "        for term in unique_terms:\n",
        "            df[term] += 1\n",
        "\n",
        "    avgdl = sum(doc_len.values()) / len(doc_len)\n",
        "    return tf, df, doc_len, avgdl\n"
      ],
      "metadata": {
        "id": "-mufL-Bvs2Iu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bm25_score(query, documents, tf, df, doc_len, avgdl, k1=1.5, b=0.75):\n",
        "    scores = defaultdict(float)\n",
        "    N = len(documents)\n",
        "\n",
        "    for term in preprocess(query):\n",
        "        if term not in df:\n",
        "            continue\n",
        "\n",
        "        idf = log((N - df[term] + 0.5) / (df[term] + 0.5))\n",
        "\n",
        "        for doc in documents:\n",
        "            f = tf[term][doc]\n",
        "            denom = f + k1 * (1 - b + b * (doc_len[doc] / avgdl))\n",
        "            scores[doc] += idf * (f * (k1 + 1)) / denom\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "ViDv4fbns97D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_collection_model(documents):\n",
        "    coll_tf = defaultdict(int)\n",
        "    total_terms = 0\n",
        "\n",
        "    for tokens in documents.values():\n",
        "        for term in tokens:\n",
        "            coll_tf[term] += 1\n",
        "            total_terms += 1\n",
        "\n",
        "    return {term: coll_tf[term] / total_terms for term in coll_tf}"
      ],
      "metadata": {
        "id": "WlBw4B71s_-w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jm_score(query, documents, tf, doc_len, collection_model, lamb=0.2):\n",
        "    scores = defaultdict(float)\n",
        "\n",
        "    for term in preprocess(query):\n",
        "        for doc in documents:\n",
        "            p_doc = tf[term][doc] / doc_len[doc] if term in tf else 0\n",
        "            p_coll = collection_model.get(term, 0)\n",
        "\n",
        "            prob = (1 - lamb) * p_doc + lamb * p_coll\n",
        "\n",
        "            if prob > 0:\n",
        "                scores[doc] += log(prob)\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "rQ_eRIvSs_8i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank(scores, top_k=3):\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n"
      ],
      "metadata": {
        "id": "7Bu0zczUs_6D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = load_documents()\n",
        "tf, df, doc_len, avgdl = compute_stats(documents)\n",
        "collection_model = build_collection_model(documents)\n",
        "\n",
        "queries = [\n",
        "    \"bm25 scoring function\",\n",
        "    \"jelinek mercer smoothing\",\n",
        "    \"document ranking method\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(f\"\\n=== Query: {q} ===\")\n",
        "\n",
        "    bm25 = bm25_score(q, documents, tf, df, doc_len, avgdl)\n",
        "    jm = jm_score(q, documents, tf, doc_len, collection_model)\n",
        "\n",
        "    print(\"\\nBM25 Results:\")\n",
        "    for doc, score in rank(bm25):\n",
        "        print(f\"{doc}: {score}\")\n",
        "\n",
        "    print(\"\\nJM Results:\")\n",
        "    for doc, score in rank(jm):\n",
        "        print(f\"{doc}: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N0X24XDs_39",
        "outputId": "57eccc90-70a9-4470-b6df-ff6ed59b9384"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Query: bm25 scoring function ===\n",
            "\n",
            "BM25 Results:\n",
            "language_model_jm.txt: 0.0\n",
            "bm25_vs_jm.txt: -0.9945277630842297\n",
            "okapi_bm25.txt: -1.3939479256595166\n",
            "\n",
            "JM Results:\n",
            "bm25_vs_jm.txt: -13.855858696845942\n",
            "okapi_bm25.txt: -16.44609865549144\n",
            "language_model_jm.txt: -20.12231273619019\n",
            "\n",
            "=== Query: jelinek mercer smoothing ===\n",
            "\n",
            "BM25 Results:\n",
            "okapi_bm25.txt: 0.0\n",
            "bm25_vs_jm.txt: -0.6894578971074721\n",
            "language_model_jm.txt: -0.8758760695268186\n",
            "\n",
            "JM Results:\n",
            "language_model_jm.txt: -3.8524040596624904\n",
            "bm25_vs_jm.txt: -4.439786438058282\n",
            "okapi_bm25.txt: -6.061456918928017\n",
            "\n",
            "=== Query: document ranking method ===\n",
            "\n",
            "BM25 Results:\n",
            "bm25_vs_jm.txt: -1.7987404739166764\n",
            "language_model_jm.txt: -2.883576697563832\n",
            "okapi_bm25.txt: -4.534512972578895\n",
            "\n",
            "JM Results:\n",
            "language_model_jm.txt: -13.977622928959965\n",
            "okapi_bm25.txt: -15.368410366528149\n",
            "bm25_vs_jm.txt: -19.015189757854166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CjqM1Jm3s9vQ"
      }
    }
  ]
}